# Chatbot Service (`services/chatbot`)

This directory contains the Python source code for the Real Madrid AI Chatbot service. It's built using FastAPI, powered by Google Gemini, and designed for serverless deployment on AWS Lambda using Mangum.

## Purpose

This service provides a simple chatbot interface that answers questions related to Real Madrid Football Club. It uses Google's Gemini language model (`gemini-2.0-flash-lite`) to generate responses, with prompt engineering designed to restrict answers to the specified topic.

## API Endpoints

The service exposes the following endpoints (defined in `app/main.py`):

* **`GET /`**
  * Description: Root endpoint for health checks.
  * Response: `{"message": "Real Madrid Chatbot (Gemini) API is running!"}`

* **`POST /chat`**
  * Description: Sends a user prompt to the chatbot and receives a generated response.
  * Request Body (JSON):

        ```json
        {
          "prompt": "Your question about Real Madrid"
        }
        ```

  * Success Response (JSON):

        ```json
        {
          "response": "The chatbot's answer..."
        }
        ```

  * Error Response (Example):

        ```json
        {
          "response": "Sorry, I couldn't generate a response."
        }
        ```

        *(Note: If the input prompt is not about Real Madrid, the expected response is: `{"response": "I'm only able to answer Real Madrid-related questions."}`)*

## Core Logic

1. Receives a `POST` request at `/chat` containing the user's `prompt`.
2. The `generate_response` function in `app.api.chatbot_api` takes the prompt.
3. It constructs a specific prompt for the Gemini model, instructing it to act as a Real Madrid expert and refuse off-topic questions.
4. It calls the configured Gemini model (`gemini-2.0-flash-lite`) using the `google-generativeai` library.
5. Handles potential API errors.
6. Returns the text response generated by Gemini.

## Dependencies

* **External Services:**
  * Google Gemini API (requires an API key).
* **Python Libraries:**
  * FastAPI: Web framework.
  * Mangum: ASGI adapter for AWS Lambda + API Gateway.
  * google-generativeai: Google's library for interacting with Gemini models.
  * Pydantic: For data validation (used by FastAPI).
  * python-dotenv: (Implied for local development) For loading environment variables.
  * Poetry: (Used for project dependency management).
  * *(See project's `pyproject.toml` for full list)*
* **AWS Services (Provisioned by Terraform in `infra/chatbot`):**
  * AWS Lambda (for execution).
  * Amazon API Gateway V2 (HTTP API) (for exposure).
  * Amazon ECR (for hosting the Docker image).
  * Amazon CloudWatch (for logging and monitoring).
  * AWS IAM (for permissions).

## Environment Variables

The service requires the following environment variable:

* `GEMINI_API_KEY`: Your API key for accessing the Google Gemini service.

This variable is loaded via `os.getenv` in `app/api/chatbot_api.py`. For local development, it can be set in a `.env` file in the project root (if using `python-dotenv` implicitly or explicitly elsewhere) or as a system environment variable. For AWS deployment, it's configured in the Lambda function's environment settings via Terraform (`infra/chatbot/main.tf`).

## Local Development & Setup

1. **Environment:** Python and Poetry installed.
2. **Dependencies:** Run `poetry install` from the project root.
3. **Configuration:** Create a `.env` file in the project root (or appropriate location) or export the `GEMINI_API_KEY` environment variable:

    ```bash
    export GEMINI_API_KEY="your_gemini_api_key"
    ```

    *(Or add `GEMINI_API_KEY=your_gemini_api_key` to a `.env` file if using python-dotenv)*.
4. **Run Locally:** From the `services/chatbot` directory, use Uvicorn:

    ```bash
    uvicorn app.main:app --reload --port 8000 # Or another suitable port
    ```

    Access the API at `http://localhost:8000`. You can test the `/chat` endpoint using tools like `curl` or Postman.

## Deployment

* **Infrastructure:** Managed via Terraform in the `infra/chatbot` directory.
* **Orchestration:** The `Makefile` in this directory (`services/chatbot`) orchestrates the deployment:
  * `make build`: Builds the Docker image using the `Dockerfile`.
  * `make tag`: Tags the image for ECR.
  * `make login`: Logs Docker into AWS ECR.
  * `make push`: Pushes the image to ECR.
  * `make deploy-ecr`: Creates the ECR repository via Terraform.
  * `make deploy-all`: Deploys the rest of the infrastructure via Terraform.
  * `make monitoring`, `make alerts`: Applies monitoring resources via Terraform.
  * `make all`: Runs the full build, push, deploy sequence.
  * `make destroy`: Destroys infrastructure and attempts ECR cleanup.
* The `Dockerfile` copies the application code and installs dependencies using Poetry within the Lambda base image. The Lambda handler is specified as `app.main.handler`.

## Folder Structure

* `app/`: Contains the core application code.
  * `api/`: Logic related to external API calls (Gemini) (`chatbot_api.py`).
  * `main.py`: FastAPI application setup, endpoint definitions, Mangum handler.
* `Dockerfile`: Instructions to build the container image for AWS Lambda.
* `Makefile`: Commands for building, deploying, and managing the service and its infrastructure.
